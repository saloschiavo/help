---
title: "Stat 437 Project 1"
author: 
     - "NAME (SID)"
header-includes:
   - \usepackage{bbm}
   - \usepackage{amssymb}
   - \usepackage{amsmath}
   - \usepackage{graphicx,float}
   - \usepackage{natbib}
output:
  pdf_document: default
fontsize: 11pt
---

```{r, echo=FALSE, warning=FALSE}
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)
```

# General rule and information
Due by 11:59PM, March 27, 2021. You must show your work in order to get points. Please prepare your report according to the rubrics on projects that are given in the syllabus. In particular, please note that your need to submit codes that would have been used for your data analysis. Your report can be in .doc, .docx, .html or .pdf format. 

The project will assess your skills in	K-means clustering,Hierarchical clustering, Nearest-neighbor classifier, and discriminant analysis for classification, for which visualization techniques you have learnt will be used to illustrate your findings. 

# Data set and its description

Please download the data set "TCGA-PANCAN-HiSeq-801x20531.tar.gz" from the website https://archive.ics.uci.edu/ml/machine-learning-databases/00401/. A brief description of the data set is given at https://archive.ics.uci.edu/ml/datasets/gene+expression+cancer+RNA-Seq. 

You need to decompress the data file since it is a .tar.gz file. Once uncompressed, the data files are "labels.csv" that contains the cancer type for each sample, and "data.csv" that contains the "gene expression profile" (i.e., expression measurements of a set of genes) for each sample. Here each sample is for a subject and is stored in a row of "data.csv". In fact, the data set contains the gene expression profiles for 801 subjects, each with a cancer type, where each gene expression profile contains the gene expressions for the same set of 20531 genes. The cancer types are: "BRCA", "KIRC", "COAD", "LUAD" and "PRAD". In both files "labels.csv" and "data.csv", each row name records which sample a label or observation is for.  


# Task A. Clustering

For this task, you need to apply k-means and hierarchical clustering to cluster observations into their associated cancer types, and report your findings scientifically and professionally. Your laptop may not have sufficient computational power to implement k-means and hierarchical clustering on the whole data set, and genes whose expressions are zero for most of the subjects may not be so informative of a cancer type.

Please use `set.seed(123)` for random sampling via the command `sample`, random initialization of `kmeans`, implementing the gap statistic, and any other process where artificial randomization is needed.

(**Task A1**) Complete the following data processing steps:
* Filter out genes (from "data.csv") whose expressions are zero for at least 300 subjects, and save the filtered data as R object "gexp2".

* Use the command `sample` to randomly select 1000 genes and their expressions from "gexp2", and save the resulting data as R object "gexp3".

* Use the command `sample` to randomly select 30 samples and their labels from the file "labels.csv", and save them as R object "labels1". For these samples, select the corresponding samples from "gexp3" and save them as R object "gexpProj1". 

* Use the command `scale` to standard the gene expressions for each gene in "gexpProj1", so that they have sample standard deviation 1. Save the standardized data as R object "stdgexpProj1".

```{r A1}
library(dplyr)
library(tidyverse)

df <- read.csv("data.csv", header=TRUE)

# Set.seed for random sampling
set.seed(123)

# Filter out genes from data.csv whose expressions are 0 for at least 300 subjects as object gexp2
gexp2 <- df %>% select_if(colSums(df==0) < 300)

# Use command sample to randomly select 1000 genes
nc = ncol(gexp2)
gsel = sample(1:nc, 1000)
# selected data; save them if needed
gexp3 = gexp2[,gsel]

# read cancer types
cancertype = read.csv("labels.csv", row.names=1, header=TRUE)
# get labels and select 30 samples randomly
subsample = sample(1:nrow(cancertype), 30)
# labels and measurements for 30 random samples
labels1 = cancertype[subsample,]
# select the corresponding samples from gexp3 and save that as object gexpProj1
gexpProj1 = gexp3[subsample,]
# View(gexpProj1)

# Standardize data so that standard deviation is 1, and save as R object stdgexpProj1
stdgexpProj1 = scale(gexpProj1) #standardize data
#View(stdgexpProj1)
```


(**Task A2**) 

(**Part 1 of Task A2**) Randomly pick 50 genes and their expressions from "stdgexpProj1", and do the following to these expressions: apply the "gap statistic" to estimate the number of clusters, apply K-means clustering with the estimated number of clusters given by the gap statistic, visualize the classification results using techniques given by "LectureNotes3_notes.pdf", and provide a summary on classification errors. You may use the command `table` and "labels1" to obtain classification errors. Note that the cluster numbering given by `kmeans` will usually be coded as follows:

```
#   Class  label
#     PRAD  5
#     LUAD  4
#     BRCA  1
#     KIRC  3
#     COAD  2
```

When you apply `clusGap`, please use arguments `K.max=10, B=200,iter.max=100`, and when you use `kmeans`, please use arguments `iter.max = 100, nstart=25, algorithm = c("Hartigan-Wong")`.

```{R A2_1}
library(cluster)
library(plotly)
library(ggplot2)
set.seed(123)

stdgexpProj1df = as.data.frame(stdgexpProj1)
subsample2=sample(stdgexpProj1df, size=50, replace=FALSE)
gap=clusGap(subsample2, kmeans, K.max=10, B=200, nstart=25, iter.max=100)
k=maxSE(gap$Tab[, "gap"], gap$Tab[,"SE.sim"], method="Tibs2001SEmax")
km.out=kmeans(subsample2,1,iter.max=100, nstart=25, algorithm=c("Hartigan-Wong"))

km.out$cluster

#View(subsample2)

# Summary of classification errors
# In this case, K-means clustering with cluster gap results in a single k cluster.
# This approach illustrates that the cluster gap is not always an accurate measure.
# This is because normally a particular type of null distribution is desired when utilizing this approach to clustering,
# but oftentimes real data will violate the ideal distribution. This is one such case. 
# As a result of our singular cluster, we can now confirm that the cluster gap is not an effective method to examine this dataset.

# table() allows us to see the count of samples for each type of cancer
table(labels1)
```

(**Part 2 of of Task A2**) Upon implementing `kmeans` with $k$ as the number of clusters, we will obtain the "total within-cluster sum of squares" $W\left(k\right)$ from the output `tot.withinss` of `kmeans`. If we try a sequence of $k=1,2,3,...,10$, then we get $W\left(k\right)$ for
each $k$ between $1$ and $10$. Let us look at the difference
$\Delta_{k}=W\left(  k\right)  -W\left(  k+1\right)$ for $k$ ranging from $1$ to $9$. The $K^{\ast}$ for which
$$
\left\{\Delta_{k}:k<K^{\ast}\right\}  \gg\left\{  \Delta_{k}:k\geq K^{\ast}\right\}
$$
is an estimate of the true number $K$ of clusters in the data, where $\gg$ means "much larger". Apply this method to obtain an estimate of $K$ for the data you created in **Part 1**, and provide a plot of $W\left(k\right)$ against $k$ for each $k$ between $1$ and $10$. Compare this estimate with the estimate obtained in **Part 1** given by the gap statistic, comment on the accuracy of the two estimates, and explain why they are different.
```{R A2_2}
library(cluster)
set.seed(123)

# labels and measurements for 50 random samples
stdgexpProj1df = as.data.frame(stdgexpProj1)
subsample3=sample(stdgexpProj1df, size=50, replace=FALSE)

# Obtain the total within-cluster sum of squares W(k) from the output tot.withinss of kmeans
# with k ranging from 1-9
km.out=kmeans(subsample3,1,iter.max=100, nstart=25, algorithm=c("Hartigan-Wong"))
km.out$tot.withinss
km.out=kmeans(subsample3,2,iter.max=100, nstart=25, algorithm=c("Hartigan-Wong"))
km.out$tot.withinss
km.out=kmeans(subsample3,3,iter.max=100, nstart=25, algorithm=c("Hartigan-Wong"))
km.out$tot.withinss
km.out=kmeans(subsample3,4,iter.max=100, nstart=25, algorithm=c("Hartigan-Wong"))
km.out$tot.withinss
km.out=kmeans(subsample3,5,iter.max=100, nstart=25, algorithm=c("Hartigan-Wong"))
km.out$tot.withinss
km.out=kmeans(subsample3,6,iter.max=100, nstart=25, algorithm=c("Hartigan-Wong"))
km.out$tot.withinss
km.out=kmeans(subsample3,7,iter.max=100, nstart=25, algorithm=c("Hartigan-Wong"))
km.out$tot.withinss
km.out=kmeans(subsample3,8,iter.max=100, nstart=25, algorithm=c("Hartigan-Wong"))
km.out$tot.withinss
km.out=kmeans(subsample3,9,iter.max=100, nstart=25, algorithm=c("Hartigan-Wong"))
km.out$tot.withinss

# Comment on the accuracy of the two estimates, explain why they are different:
# As the number of clusters k increases, the total within-cluster sum of squares decreases
# The first k-clustering model uses the cluster gap, which is not always accurate when
# working data sets from real life situations.

# Provide a plot of W(k) against k for each k between 1 and 10
#ggplot(data=subsample3,aes(km.out, km.out$tot.withinss, colour='blue'))+geom_point()+theme_bw()

```


(**Part 3 of of Task A2**) Randomly pick 250 genes and their expressions from "stdgexpProj1", and for these expressions, do the analysis in **Part 1** and **Part 2**. Report your findings, compare your findings with those from **Part 1** and **Part 2**; if there are differences between these findings, explain why. Regard using more genes as using more features, does using more features necessarily give more accurate clustering or classification results? 
```{R A2-3}
set.seed(123)
stdgexpProj1df = as.data.frame(stdgexpProj1)
subsample4=sample(stdgexpProj1df, size=250, replace=FALSE)
gap=clusGap(subsample4, kmeans, K.max=10, B=200, nstart=25, iter.max=100)
k=maxSE(gap$Tab[, "gap"], gap$Tab[,"SE.sim"], method="Tibs2001SEmax")
km.out=kmeans(subsample4,k,iter.max=100, nstart=25, algorithm=c("Hartigan-Wong"))
km.out$cluster
# This still provides us with a single cluster from the cluster gap

# K-means clustering with k=1-9
stdgexpProj1df = as.data.frame(stdgexpProj1)
subsample3=sample(stdgexpProj1df, size=250, replace=FALSE)

# Obtain the total within-cluster sum of squares W(k) from the output tot.withinss of kmeans
# with k ranging from 1-9
km.out=kmeans(subsample4,1,iter.max=100, nstart=25, algorithm=c("Hartigan-Wong"))
km.out$tot.withinss
km.out=kmeans(subsample4,2,iter.max=100, nstart=25, algorithm=c("Hartigan-Wong"))
km.out$tot.withinss
km.out=kmeans(subsample4,3,iter.max=100, nstart=25, algorithm=c("Hartigan-Wong"))
km.out$tot.withinss
km.out=kmeans(subsample4,4,iter.max=100, nstart=25, algorithm=c("Hartigan-Wong"))
km.out$tot.withinss
km.out=kmeans(subsample4,5,iter.max=100, nstart=25, algorithm=c("Hartigan-Wong"))
km.out$tot.withinss
km.out=kmeans(subsample4,6,iter.max=100, nstart=25, algorithm=c("Hartigan-Wong"))
km.out$tot.withinss
km.out=kmeans(subsample4,7,iter.max=100, nstart=25, algorithm=c("Hartigan-Wong"))
km.out$tot.withinss
km.out=kmeans(subsample4,8,iter.max=100, nstart=25, algorithm=c("Hartigan-Wong"))
km.out$tot.withinss
km.out=kmeans(subsample4,9,iter.max=100, nstart=25, algorithm=c("Hartigan-Wong"))
km.out$tot.withinss

# Do more genes/features necessarily gives more accurate clustering or classification results?
# The results using subsample4 (n=250) are less accurate results than the previous results from the subsample3 (n=50),
# so no, using more genes/features does not improve the clustering results. The total within-cluster sum of squares again
# decreases as k clusters increase; however, the sum of squares is much higher in all of the results where n=250.

```


(**Task A3**) Randomly pick 250 genes and their expressions from "stdgexpProj1", and for these expressions, do the following: respectively apply hierarchical clustering with average linkage, single linkage, and complete linkage to cluster subjects into groups, and create a dendrogram. For the dendrogram obtained from average linkage, find the height at which cutting the dendrogram gives the same number of groups in "labels1", and comment on the clustering results obtained at this height by comparing them to the truth contained in "labels1".
```{R RA-3}
library(cluster)
library(ggdendro)
set.seed(123)
stdgexpProj1df2 = as.data.frame(stdgexpProj1)
subsample3=sample(stdgexpProj1df2, size=250, replace=FALSE)
subsample3

# obtain dissimilarity
dMat = dist(scale(t(subsample3)))
length(dMat)
# Apply hierarchical clustering with avg linkage
hc.average=hclust(dMat, method="average")
ggdendrogram(hc.average, rotate=F)

# Apply hierarchical clustering with single linkage
hc.single=hclust(dMat, method="single")
ggdendrogram(hc.single, rotate=F)

# Apply hierarchical clustering with complete linkage
hc.complete=hclust(dMat, method="complete")
ggdendrogram(hc.complete,rotate=F)

cutree(hc.complete, 4)
cutree(hc.single, 4)
cutree(hc.average, 4)

# TODO: For avg linkage dendrogram, find height at which cutting the dendrogram gives the same number of groups in labels1
# TODO: Comment on clustering results obtained at this height by comparing them to the truth contained in labels1
```

# Task B. Classification

For this task, we will use the same data set you would have downloaded. Please use `set.seed(123)` for random sampling via the command `sample` and any other process where artificial randomization is needed.

(**Task B1**) After you obtain "labels.csv" and "data.csv", do the following:

*  Filter out genes (from "data.csv") whose expressions are zero for at least 300 subjects, and save the filtered data as R object "gexp2".

*  Use the command `sample` to randomly select 1000 genes and their expressions from "gexp2", and save the resulting data as R object "gexp3". 

*  Pick the samples from "labels.csv" that are for cancer type "LUAD" or "BRCA", and save them as object "labels2". For these samples, pick the corresponding gene expressions from "gexp3" and save them as object "stdgexp2"

```{r B1}
library(dplyr)
library(tidyverse)
df <- read.csv("data.csv", header=TRUE)
set.seed(123)

# Filter out genes whose expressions are zero for at least 300 subjects, save the filtered data as gexp2
gexp2 <- df %>% select_if(colSums(df==0) < 300)

# Use command sample to randomly select 1000 genes and their expressions from gexp2, save resulting data as gexp3
nc = ncol(gexp2)
gsel = sample(1:nc, 1000)
# selected data; save them if needed
gexp3 = gexp2[,gsel]

# read cancer types
cancertype = read.csv("labels.csv", row.names=1, header=TRUE)
# get labels and select 30 samples randomly
subsample = sample(1:nrow(cancertype), 30)
# labels and measurements for 30 random samples
labels1 = cancertype[subsample,]
# select the corresponding samples from gexp3 and save that as object gexpProj1
stdgexp2 = gexp3[subsample,]



#data(cancertype); n=dim(cancertype)[2]; p=dim(cancertype)[1] # get dimensions
#rSel=sample(1:p, size=1000, replace=FALSE)
#chk=colnames(cancertype) %in% c("LUAD", "BRCA")
#cSel = which(chk==TRUE)
#ncia=nci[rSel, cSel]
#colNames(ncia) = colnames(cancertype)[cSel]

# TODO: Save as labels2

# Pick samples from 'labels.csv' that are for cancer type "LUAD" or "BRCA", save as labels2
# TODO: FIX this, it currently doesn't work because labels isn't a compatible data type
#labels2 <- labels1 %in% select_if(col=c("LUAD", "BRCA"))


# Pick corresponding gene expressions from gexp3 and save them as object stdgexp2



```

(**Taks B2**) The assumptions of linear or quadratic discriminant analysis requires that each observation follows a Gaussian distribution given the class or group membership of the observation, and that each observation follows a Gaussian mixture model. In our settings here, each observation (as a row) within a group would follow a Gaussian with dimensionality equal to the number of genes (i.e., number of entries of the row). So, the more genes whose expressions we use for classification, the higher the dimension of these Gaussian distributions. Nonetheless, you need to check if the Gaussian mixture assumption is satisfied. Note that we only consider two classes "LUAD" and "BRCA", for which the corresponding Gaussian mixture has 2 components and hence has 2 bumps when its density is plotted.

Do the following and report your findings on classification:

* Randomly pick 3 genes and their expressions from "stdgexp2", and save them as object "stdgexp2a".

* Randomly pick 60% of samples from "stdgexp2a", use them as the training set, and use the rest as the test set. You can round down the number of samples in the training set by the command `floor` if it is not an integer.


Build a quadratic discriminant analysis model using the training set, and apply the obtained model to the test set to classify each of its observations. You should code "BRCA" as 0 and "LUAD" as 1. If for an observation the posterior probability of being "BRCA" is predicted by the model to be greater than 0.5, the observation is classified as "BRCA". Report via a 2-by-2 table on the classification errors. Note that the predicted posterior probability given by `qda` is for an observation to belong to class "BRCA".

Before building a quadratic discriminant analysis model, you need to check for highly correlated gene expressions, i.e., you need to check the sample correlations between each pair of columns of the training set. If there are highly correlated gene expressions, the estimated covariance matrix can be close to to being singular, leading to unstable inference. You can remove a column from two columns when their contained expressions have sample correlation greater than 0.9 in absolute value.
```{r B2}
set.seed(123)

```


(**Task B3**) Do the following:

* Randomly pick 100 genes and their expressions from "stdgexp2", and save them as object "stdgexp2b".

* Randomly pick 75% of samples from "stdgexp2b", use them as the training set, and use the rest as the test set. You can round down the number of samples in the training set by the command `floor` if it is not an integer.

Then apply quadratic discriminant analysis by following the requirements given in **Task B2**. Compare classification results you find here with those found in **Task B2**, and explain on any difference you find between the classification results.
```{r B3}
set.seed(123)


#I you get an error  "Error in qda.default(x, grouping, ...) : some group is too small for 'qda'" while implementing `qda` in Task B3. Your error message suggests that some of the estimates (such as the covariance matrix for each component density) are singular, which was discussed in lecture notes and lecture videos. In this case, the first step is to look into the data to see if there are some unusual aspects such as highly correlated features, and then see if some adjustment can be done, not necessarily by reducing the number of features (imaging that some features are what your colleagues want to check on and cannot be removed from the model).

#Assuming your library `MASS` is the same version as mine as '7.3.49'. You can study and try out the following codes:

----------

#stdgexp2b = stdgexp2[,sample(1:ncol(stdgexp2),100)]
#n1 = nrow(stdgexp2b)

 
# obtain subsets of data

#set.seed(123)
#trainid = sample(1:n1,floor(0.75*n1))
#testid = (1:n1)[-trainid]

#trainOB = stdgexp2b[trainid,]
#testOB = stdgexp2b[testid,]
 
# relabel: note 1=BRCA, 4=LUAD for
#labels2a = as.numeric(labels2)
#labels2a[labels2a == 1] = 0
#labels2a[labels2a == 4] = 1
#trainLb = labels2a[trainid]
#testLb = labels2a[testid]

# check correlations and visualize them via a heatmap
#corMat = cor(trainOB)
#library(reshape2)
#melted_cormat <- melt(corMat)
#head(melted_cormat)


#library(ggplot2)
#ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) +
# geom_tile()+scale_fill_gradient2(low = "blue", high = "red")


# check on  highly correlated variables
#corMat1 = corMat
# set diagonal entries of corMat1 to be 0 since they are 1

#diag(corMat1) = 0
#hcIdx = which(abs(corMat1) > 0.9, arr.ind = TRUE)
#hcIdx

# extract X and Y from test and training data
#testX = as.matrix(testOB)
#testY = as.numeric(testLb)
#trainX = as.matrix(trainOB)
#trainY = as.numeric(trainLb)

# combine labels and expressions
#trainOB1 = as.data.frame(cbind(trainLb,trainOB))

# quadratic discriminant analysis
#library(MASS)
#qda.fit=qda(trainLb ~ .,data=trainOB1)

```

(**Task B4**) Do the following:

* Randomly pick 100 genes and their expressions from "stdgexp2", and save them as object "stdgexp2b".

* Randomly pick 75% of samples from "stdgexp2b", use them as the training set, and use the rest as the test set. You can round down the number of samples in the training set by the command `floor` if it is not an integer.

Then apply k-nearest-neighbor (k-NN) method with neighborhood size k=3 to the test data to classify each observation in the test set into one of the cancer types. Here, for an observation, if the average of being cancer type "BRCA" is predicted by k-NN to be greater than 0.5, then the observation is classified as being "BRCA". Report via a 2-by-2 table on the classification errors. Compare and comment on the classification results obtained here to those obtain in **Taks B3**. If there is any difference between the classification results, explain why. 
```{r B4}
set.seed(123)

```
